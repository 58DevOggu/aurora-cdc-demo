# Aurora CDC Demo - Quick Start Guide

## ğŸš€ One-Click Demo Setup for Developers

This demo creates a complete CDC pipeline from Aurora MySQL (500+ tables) to Databricks Delta Lake using a custom PySpark DataSource.

## Prerequisites

### Required
- AWS Account with appropriate permissions
- Databricks Workspace (existing, on separate VPC)
- Python 3.8+
- AWS CLI configured
- 10GB free storage in Unity Catalog

### AWS Permissions Needed
- VPC and networking (create/modify)
- RDS Aurora (create/modify)
- Secrets Manager (create/read)
- CloudFormation (create/delete stacks)
- S3 (create buckets)

## ğŸ¯ Quick Start (5 Minutes)

### Step 1: Clone and Setup
```bash
git clone https://github.com/yourusername/aurora-cdc-demo.git
cd aurora-cdc-demo/demo/setup

# Install requirements
pip install -r ../requirements/demo.txt

# Make scripts executable
chmod +x demo_setup.sh cleanup_demo.sh
```

### Step 2: Configure Environment
```bash
# Set your AWS region
export AWS_REGION=us-east-1

# Set demo name (optional)
export DEMO_NAME=aurora-cdc-demo

# Set TPC-H scale (1 = 1GB, 10 = 10GB)
export TPCH_SCALE=1
```

### Step 3: Run Setup
```bash
./demo_setup.sh
```

You'll be prompted for:
- Databricks VPC ID
- Databricks VPC CIDR (e.g., 10.1.0.0/16)
- Databricks AWS Account ID
- Databricks Region

The script will:
1. âœ… Deploy Aurora MySQL cluster with CDC enabled
2. âœ… Create 500+ tables with TPC-H data
3. âœ… Setup cross-VPC peering with Databricks
4. âœ… Start CDC data generator (creates continuous changes)
5. âœ… Generate Databricks notebook configured for your environment
6. âœ… Run validation tests

### Step 4: Import Databricks Notebook
1. Open your Databricks workspace
2. Import the generated notebook: `../notebooks/demo/unity_catalog_cdc_demo.py`
3. Attach to a cluster with:
   - Spark 3.5+ / DBR 13.3+
   - Unity Catalog enabled
   - Network access to Aurora VPC

### Step 5: Configure Databricks Secrets
```python
# In Databricks, create secrets scope
databricks secrets create-scope --scope aurora_cdc

# Add Aurora credentials (get from demo output)
databricks secrets put --scope aurora_cdc --key host
databricks secrets put --scope aurora_cdc --key port
databricks secrets put --scope aurora_cdc --key username
databricks secrets put --scope aurora_cdc --key password
```

### Step 6: Run CDC Pipeline
In the Databricks notebook:
1. Run Setup cells (creates Unity Catalog tables)
2. Run Initial Load (optional, for existing data)
3. Start CDC Streaming (real-time changes)
4. Monitor dashboard

## ğŸ“Š What's Included

### Aurora MySQL Setup
- **Cluster**: db.r6g.xlarge instances (configurable)
- **Tables**: 500+ tables across 10 business domains
- **Data**: TPC-H dataset with continuous updates
- **CDC**: Binary logging optimized for streaming

### Data Generation
- **Rate**: 100-1000 changes/second
- **Operations**: INSERT, UPDATE, DELETE
- **Scenarios**: 10 realistic business scenarios
- **Distribution**: Balanced across all tables

### CDC Features
- **Parallelism**: 20 concurrent table processors
- **Batch Size**: 10,000 records
- **Interval**: 15-minute cycles (configurable)
- **Mode**: Real-time with 100ms triggers

### Unity Catalog Integration
- **Catalog**: `cdc_demo`
- **Schema**: `tpch`
- **Volume**: `/Volumes/cdc_demo/tpch/cdc_data`
- **Tables**: All stored as managed Delta tables
- **Features**: Change Data Feed, Time Travel, Z-Ordering

## ğŸ§ª Testing

### Run Integration Tests
```bash
cd aurora-cdc-demo
python tests/test_cdc_integration.py \
  --host <aurora-endpoint> \
  --username cdcadmin \
  --password <password>
```

Tests verify:
- âœ… Aurora connectivity
- âœ… VPC peering
- âœ… 500+ tables exist
- âœ… CDC captures all operations
- âœ… Performance > 1000 events/sec
- âœ… Data integrity

## ğŸ“ˆ Monitoring

### CDC Generator Logs
```bash
# View real-time CDC generation
tail -f cdc_generator.log

# Check generator status
ps aux | grep cdc_data_generator
```

### Aurora Metrics
- CloudWatch Dashboard (auto-created)
- Binary log position
- Replication lag
- Connection count

### Databricks Metrics
- Streaming query progress
- Records processed/second
- Checkpoint status
- Delta table statistics

## ğŸ› ï¸ Customization

### Adjust CDC Rate
```bash
# Edit generator parameters
python ../scripts/cdc_data_generator.py \
  --interval 1 \        # Seconds between batches
  --batch-size 500      # Changes per batch
```

### Scale Tables
```bash
# Modify table count in setup script
TPCH_SCALE=10 ./demo_setup.sh  # 10GB dataset
```

### Change Streaming Interval
In Databricks notebook:
```python
streaming_config = {
    "fetch_interval_seconds": "300",  # 5 minutes
    "real_time_mode": "false"         # Batch mode
}
```

## ğŸ§¹ Cleanup

### Complete Cleanup
```bash
./cleanup_demo.sh
```

This removes:
- Aurora cluster and data
- VPC and networking
- Secrets and S3 buckets
- Local configuration

**Note**: Delta tables in Databricks are NOT deleted automatically.

### Partial Cleanup
```bash
# Stop CDC generator only
kill $(cat cdc_generator.pid)

# Delete Aurora only (keep VPC)
aws cloudformation delete-stack --stack-name aurora-cdc-demo-aurora
```

## ğŸ”§ Troubleshooting

### Connection Issues
```bash
# Test Aurora connectivity
mysql -h <aurora-endpoint> -P 3306 -u cdcadmin -p

# Check VPC peering
aws ec2 describe-vpc-peering-connections

# Verify security groups
aws ec2 describe-security-groups --group-ids <sg-id>
```

### CDC Not Working
```sql
-- Check binlog settings
SHOW VARIABLES LIKE 'log_bin';
SHOW VARIABLES LIKE 'binlog_format';

-- Check CDC user permissions
SHOW GRANTS FOR 'cdc_user'@'%';
```

### Performance Issues
```python
# In Databricks, check streaming metrics
display(spark.streams.active[0].lastProgress)

# Optimize Delta tables
spark.sql("OPTIMIZE cdc_demo.tpch.orders ZORDER BY (updated_at)")
```

## ğŸ“š Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Aurora VPC    â”‚ Peering â”‚ Databricks VPC  â”‚
â”‚                 â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚         â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Aurora   â”‚  â”‚         â”‚  â”‚  Spark    â”‚  â”‚
â”‚  â”‚  MySQL    â”‚  â”‚         â”‚  â”‚  Cluster  â”‚  â”‚
â”‚  â”‚  (500+    â”‚  â”‚   CDC   â”‚  â”‚           â”‚  â”‚
â”‚  â”‚  tables)  â”œâ”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â–º  Custom   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚         â”‚  â”‚DataSource â”‚  â”‚
â”‚                 â”‚         â”‚  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚         â”‚        â”‚        â”‚
â”‚  â”‚   CDC     â”‚  â”‚         â”‚  â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Generator â”‚  â”‚         â”‚  â”‚   Unity   â”‚  â”‚
â”‚  â”‚           â”‚  â”‚         â”‚  â”‚  Catalog  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚         â”‚  â”‚  (Delta)  â”‚  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“ Notes

- **Cost**: ~$5-10/hour for demo resources
- **Data Volume**: ~1GB/hour of CDC data
- **Retention**: 7 days of Aurora backups
- **Scaling**: Tested up to 1000 tables
- **Latency**: < 1 second end-to-end

## ğŸ¤ Support

For issues or questions:
1. Check logs: `cdc_generator.log`, CloudWatch
2. Run tests: `python tests/test_cdc_integration.py`
3. Review notebook output in Databricks
4. File issue on GitHub

## ğŸ“„ License

MIT License - See LICENSE file

---

**Happy CDC Streaming! ğŸš€**