# Databricks notebook source
# MAGIC %md
# MAGIC # Aurora CDC for 500+ Tables with Real-Time Streaming
# MAGIC 
# MAGIC This notebook demonstrates CDC from Aurora MySQL to Delta Lake for 500+ tables using:
# MAGIC - Custom PySpark DataSource with Spark 4 features
# MAGIC - Real-time streaming with checkpointing
# MAGIC - Parallel processing for optimal performance
# MAGIC - 15-minute batch intervals with intelligent table scheduling

# COMMAND ----------

# MAGIC %md
# MAGIC ## 1. Setup and Configuration

# COMMAND ----------

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
from delta.tables import DeltaTable
import json
from datetime import datetime, timedelta
import concurrent.futures
import os

# Configure Spark session
spark = SparkSession.builder \
    .appName("Aurora CDC 500+ Tables") \
    .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
    .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog") \
    .config("spark.databricks.delta.schema.autoMerge.enabled", "true") \
    .config("spark.databricks.delta.merge.repartitionBeforeWrite.enabled", "true") \
    .config("spark.sql.streaming.stateStore.providerClass", "org.apache.spark.sql.execution.streaming.state.RocksDBStateStoreProvider") \
    .config("spark.sql.streaming.metricsEnabled", "true") \
    .config("spark.sql.streaming.lastProgress.retention", "100") \
    .config("spark.sql.adaptive.enabled", "true") \
    .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
    .getOrCreate()

print(f"Spark version: {spark.version}")
print(f"Delta Lake enabled: {spark.conf.get('spark.sql.extensions')}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 2. Configuration Parameters

# COMMAND ----------

# Database configuration
DB_CONFIG = {
    "host": dbutils.secrets.get(scope="aurora", key="host"),
    "port": "3306",
    "database": dbutils.secrets.get(scope="aurora", key="database"),
    "username": dbutils.secrets.get(scope="aurora", key="username"),
    "password": dbutils.secrets.get(scope="aurora", key="password")
}

# Streaming configuration
STREAMING_CONFIG = {
    # Table selection
    "table_pattern": "*",  # Process all tables
    "excluded_tables": "temp_*,staging_*,backup_*",  # Exclude temporary tables
    
    # Performance settings
    "parallelism": "20",  # Number of parallel connections
    "batch_size": "10000",  # Records per batch
    "max_tables_per_batch": "50",  # Tables to process in parallel
    "fetch_interval_seconds": "900",  # 15 minutes
    
    # Checkpointing
    "checkpoint_location": "/mnt/delta/checkpoints/aurora_cdc",
    "enable_exactly_once": "true",
    
    # Real-time mode
    "real_time_mode": "true",
    "max_trigger_delay_ms": "100",
    
    # Start position
    "start_position": "latest"  # or "earliest"
}

# Delta Lake configuration
DELTA_CONFIG = {
    "base_path": "/mnt/delta/aurora_cdc",
    "optimize_interval_minutes": 60,
    "vacuum_retention_hours": 168,  # 7 days
    "z_order_columns": "timestamp,table"
}

# Combine all configurations
CDC_OPTIONS = {**DB_CONFIG, **STREAMING_CONFIG}

print("Configuration loaded successfully")
display(CDC_OPTIONS)

# COMMAND ----------

# MAGIC %md
# MAGIC ## 3. Register Custom DataSource

# COMMAND ----------

# Register the custom DataSource
spark.dataSource.register("aurora_cdc_v2", "aurora_cdc.datasource.aurora_cdc_datasource_v2.AuroraCDCDataSourceV2")

print("Custom DataSource registered: aurora_cdc_v2")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 4. Create CDC Stream with Real-Time Mode

# COMMAND ----------

def create_cdc_stream():
    """Create the main CDC stream for all tables."""
    
    # Read CDC stream using custom DataSource
    cdc_stream = spark.readStream \
        .format("aurora_cdc_v2") \
        .options(**CDC_OPTIONS) \
        .load()
    
    # Add processing metadata
    cdc_stream_enriched = cdc_stream \
        .withColumn("ingestion_timestamp", current_timestamp()) \
        .withColumn("ingestion_date", to_date(col("timestamp"))) \
        .withColumn("year", year(col("timestamp"))) \
        .withColumn("month", month(col("timestamp"))) \
        .withColumn("day", dayofmonth(col("timestamp"))) \
        .withColumn("hour", hour(col("timestamp"))) \
        .withWatermark("event_time", "30 minutes")  # Watermark for late data
    
    return cdc_stream_enriched

# Create the stream
cdc_stream = create_cdc_stream()

print("CDC stream created successfully")
cdc_stream.printSchema()

# COMMAND ----------

# MAGIC %md
# MAGIC ## 5. Define Processing Logic for Different Table Types

# COMMAND ----------

def process_cdc_batch(batch_df, batch_id):
    """
    Process a micro-batch of CDC events.
    Handles different table types and operations.
    """
    print(f"Processing batch {batch_id} with {batch_df.count()} events")
    
    # Get unique tables in this batch
    tables_in_batch = batch_df.select("table").distinct().collect()
    
    # Process each table separately for optimal performance
    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
        futures = []
        
        for row in tables_in_batch:
            table_name = row["table"]
            future = executor.submit(
                process_table_events,
                batch_df.filter(col("table") == table_name),
                table_name,
                batch_id
            )
            futures.append(future)
        
        # Wait for all tables to be processed
        for future in concurrent.futures.as_completed(futures):
            try:
                future.result()
            except Exception as e:
                print(f"Error processing table: {e}")

def process_table_events(table_df, table_name, batch_id):
    """Process CDC events for a specific table."""
    
    delta_path = f"{DELTA_CONFIG['base_path']}/{table_name}"
    
    try:
        # Check if Delta table exists
        if DeltaTable.isDeltaTable(spark, delta_path):
            delta_table = DeltaTable.forPath(spark, delta_path)
            
            # Separate operations
            inserts_df = table_df.filter(col("operation") == "INSERT")
            updates_df = table_df.filter(col("operation") == "UPDATE")
            deletes_df = table_df.filter(col("operation") == "DELETE")
            
            # Process deletes
            if deletes_df.count() > 0:
                process_deletes(delta_table, deletes_df, table_name)
            
            # Process updates
            if updates_df.count() > 0:
                process_updates(delta_table, updates_df, table_name)
            
            # Process inserts
            if inserts_df.count() > 0:
                process_inserts(delta_table, inserts_df, table_name)
        else:
            # Create new Delta table
            create_delta_table(table_df, table_name, delta_path)
        
        print(f"Successfully processed {table_df.count()} events for table {table_name}")
        
    except Exception as e:
        print(f"Error processing table {table_name}: {e}")
        # Log to error table for monitoring
        log_error(table_name, batch_id, str(e))

def process_inserts(delta_table, inserts_df, table_name):
    """Process INSERT operations."""
    # Extract and transform the 'after' data
    insert_data = inserts_df.select(
        col("after").alias("data"),
        col("timestamp"),
        col("transaction_id")
    )
    
    # Flatten the JSON data
    flattened_df = flatten_json_df(insert_data, "data")
    
    # Append to Delta table
    flattened_df.write \
        .format("delta") \
        .mode("append") \
        .option("mergeSchema", "true") \
        .save(delta_table.detail().select("location").collect()[0][0])

def process_updates(delta_table, updates_df, table_name):
    """Process UPDATE operations using MERGE."""
    # Extract primary keys and new data
    update_data = updates_df.select(
        col("primary_keys"),
        col("after").alias("new_data"),
        col("timestamp")
    )
    
    # Build merge condition
    merge_condition = build_merge_condition(update_data.select("primary_keys").first()["primary_keys"])
    
    # Perform merge
    delta_table.alias("target").merge(
        update_data.alias("source"),
        merge_condition
    ).whenMatchedUpdateAll().execute()

def process_deletes(delta_table, deletes_df, table_name):
    """Process DELETE operations."""
    # Extract primary keys for deletion
    delete_keys = deletes_df.select(col("primary_keys"))
    
    # Build delete condition
    delete_condition = build_merge_condition(delete_keys.first()["primary_keys"])
    
    # Perform delete
    delta_table.delete(delete_condition)

def create_delta_table(table_df, table_name, delta_path):
    """Create a new Delta table from CDC events."""
    # Extract schema from the first 'after' record
    first_record = table_df.filter(col("after").isNotNull()).first()
    
    if first_record:
        # Create initial table from INSERT operations
        initial_data = table_df.filter(col("operation") == "INSERT").select(
            col("after").alias("data"),
            col("timestamp")
        )
        
        # Flatten and write
        flattened_df = flatten_json_df(initial_data, "data")
        flattened_df.write \
            .format("delta") \
            .mode("overwrite") \
            .option("overwriteSchema", "true") \
            .partitionBy("year", "month", "day") \
            .save(delta_path)
        
        print(f"Created new Delta table for {table_name}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 6. Helper Functions

# COMMAND ----------

def flatten_json_df(df, json_column):
    """Flatten JSON column into individual columns."""
    # Get the first non-null value to infer schema
    first_json = df.filter(col(json_column).isNotNull()).first()[json_column]
    
    if isinstance(first_json, dict):
        # Create schema from dictionary
        json_schema = spark.read.json(
            spark.sparkContext.parallelize([json.dumps(first_json)])
        ).schema
        
        # Parse JSON and expand
        return df.withColumn(json_column, from_json(col(json_column), json_schema)) \
                 .select("*", f"{json_column}.*") \
                 .drop(json_column)
    else:
        # Already flattened or simple type
        return df

def build_merge_condition(primary_keys):
    """Build merge condition from primary keys."""
    if not primary_keys:
        return "1=1"  # No primary keys, match all
    
    conditions = []
    for key, value in primary_keys.items():
        conditions.append(f"target.{key} = source.primary_keys.{key}")
    
    return " AND ".join(conditions)

def log_error(table_name, batch_id, error_message):
    """Log errors to monitoring table."""
    error_df = spark.createDataFrame(
        [(table_name, batch_id, error_message, datetime.now())],
        ["table_name", "batch_id", "error_message", "error_timestamp"]
    )
    
    error_df.write \
        .format("delta") \
        .mode("append") \
        .save(f"{DELTA_CONFIG['base_path']}/error_log")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 7. Start Real-Time Streaming with Checkpointing

# COMMAND ----------

# Define the streaming query with real-time mode
streaming_query = cdc_stream \
    .writeStream \
    .outputMode("append") \
    .foreachBatch(process_cdc_batch) \
    .trigger(processingTime="100 milliseconds") \
    .option("checkpointLocation", STREAMING_CONFIG["checkpoint_location"]) \
    .option("maxFilesPerTrigger", 100) \
    .start()

print(f"Streaming query started: {streaming_query.id}")
print(f"Status: {streaming_query.status}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 8. Monitoring Dashboard

# COMMAND ----------

def create_monitoring_dashboard():
    """Create monitoring views for the CDC pipeline."""
    
    # Create monitoring queries
    queries = []
    
    # 1. Stream progress monitoring
    progress_query = cdc_stream \
        .select("table", "operation", "timestamp", "error_flag") \
        .groupBy(
            window(col("timestamp"), "1 minute"),
            "table",
            "operation"
        ) \
        .agg(
            count("*").alias("event_count"),
            sum(when(col("error_flag") == True, 1).otherwise(0)).alias("error_count"),
            max("timestamp").alias("last_event_time")
        ) \
        .writeStream \
        .outputMode("complete") \
        .format("memory") \
        .queryName("cdc_progress") \
        .trigger(processingTime="10 seconds") \
        .start()
    
    queries.append(progress_query)
    
    # 2. Table-level statistics
    table_stats_query = cdc_stream \
        .groupBy("table") \
        .agg(
            count("*").alias("total_events"),
            countDistinct("transaction_id").alias("unique_transactions"),
            min("timestamp").alias("first_event"),
            max("timestamp").alias("last_event")
        ) \
        .writeStream \
        .outputMode("complete") \
        .format("memory") \
        .queryName("table_statistics") \
        .trigger(processingTime="30 seconds") \
        .start()
    
    queries.append(table_stats_query)
    
    # 3. Error monitoring
    error_query = cdc_stream \
        .filter(col("error_flag") == True) \
        .select("table", "error_message", "timestamp") \
        .writeStream \
        .outputMode("append") \
        .format("memory") \
        .queryName("cdc_errors") \
        .trigger(processingTime="5 seconds") \
        .start()
    
    queries.append(error_query)
    
    return queries

# Start monitoring
monitoring_queries = create_monitoring_dashboard()
print(f"Started {len(monitoring_queries)} monitoring queries")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 9. Display Real-Time Metrics

# COMMAND ----------

import time
import pandas as pd
from IPython.display import display, clear_output

def display_metrics(duration_seconds=60):
    """Display real-time metrics for the specified duration."""
    
    start_time = time.time()
    
    while time.time() - start_time < duration_seconds:
        clear_output(wait=True)
        
        print(f"=== CDC Pipeline Metrics at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} ===\n")
        
        # Stream progress
        if spark.catalog.tableExists("cdc_progress"):
            print("ðŸ“Š Stream Progress (Last Minute):")
            progress_df = spark.sql("SELECT * FROM cdc_progress ORDER BY window DESC LIMIT 10")
            display(progress_df)
        
        # Table statistics
        if spark.catalog.tableExists("table_statistics"):
            print("\nðŸ“ˆ Table Statistics:")
            stats_df = spark.sql("""
                SELECT table, total_events, unique_transactions, 
                       last_event as last_update
                FROM table_statistics 
                ORDER BY total_events DESC 
                LIMIT 20
            """)
            display(stats_df)
        
        # Errors
        if spark.catalog.tableExists("cdc_errors"):
            print("\nâŒ Recent Errors:")
            errors_df = spark.sql("SELECT * FROM cdc_errors ORDER BY timestamp DESC LIMIT 5")
            if errors_df.count() > 0:
                display(errors_df)
            else:
                print("No errors detected âœ…")
        
        # Streaming query status
        print(f"\nðŸ”„ Streaming Query Status:")
        print(f"  - Query ID: {streaming_query.id}")
        print(f"  - Is Active: {streaming_query.isActive}")
        print(f"  - Last Progress: {streaming_query.lastProgress}")
        
        time.sleep(5)

# Display metrics for 60 seconds
# display_metrics(60)

# COMMAND ----------

# MAGIC %md
# MAGIC ## 10. Performance Optimization

# COMMAND ----------

def optimize_delta_tables():
    """Optimize Delta tables for better query performance."""
    
    import os
    from delta.tables import DeltaTable
    
    # Get all Delta table paths
    base_path = DELTA_CONFIG['base_path']
    
    if os.path.exists(base_path):
        for table_dir in os.listdir(base_path):
            table_path = os.path.join(base_path, table_dir)
            
            if DeltaTable.isDeltaTable(spark, table_path):
                try:
                    delta_table = DeltaTable.forPath(spark, table_path)
                    
                    # Optimize table (compact small files)
                    print(f"Optimizing table: {table_dir}")
                    spark.sql(f"OPTIMIZE delta.`{table_path}`")
                    
                    # Z-order by frequently queried columns
                    if DELTA_CONFIG.get('z_order_columns'):
                        spark.sql(f"""
                            OPTIMIZE delta.`{table_path}` 
                            ZORDER BY ({DELTA_CONFIG['z_order_columns']})
                        """)
                    
                    # Vacuum old files
                    spark.sql(f"""
                        VACUUM delta.`{table_path}` 
                        RETAIN {DELTA_CONFIG['vacuum_retention_hours']} HOURS
                    """)
                    
                    print(f"  âœ“ Optimized {table_dir}")
                    
                except Exception as e:
                    print(f"  âœ— Error optimizing {table_dir}: {e}")

# Schedule optimization (run periodically)
# optimize_delta_tables()

# COMMAND ----------

# MAGIC %md
# MAGIC ## 11. Graceful Shutdown

# COMMAND ----------

def shutdown_streaming():
    """Gracefully shutdown all streaming queries."""
    
    # Stop monitoring queries
    for query in monitoring_queries:
        if query.isActive:
            query.stop()
            print(f"Stopped monitoring query: {query.name}")
    
    # Stop main streaming query
    if streaming_query.isActive:
        streaming_query.stop()
        print(f"Stopped main CDC streaming query")
    
    # Wait for termination
    streaming_query.awaitTermination(timeout=30)
    
    print("All streaming queries stopped successfully")

# Uncomment to shutdown
# shutdown_streaming()

# COMMAND ----------

# MAGIC %md
# MAGIC ## 12. Query Examples for Processed Data

# COMMAND ----------

# Example 1: Get latest state of a table
def get_table_latest_state(table_name):
    """Get the latest state of a specific table."""
    
    delta_path = f"{DELTA_CONFIG['base_path']}/{table_name}"
    
    if DeltaTable.isDeltaTable(spark, delta_path):
        df = spark.read.format("delta").load(delta_path)
        print(f"Table: {table_name}")
        print(f"Total Records: {df.count()}")
        print(f"Schema:")
        df.printSchema()
        return df
    else:
        print(f"Table {table_name} not found")
        return None

# Example usage
# customers_df = get_table_latest_state("customers")
# display(customers_df.limit(10))

# COMMAND ----------

# Example 2: Time travel query
def query_table_at_timestamp(table_name, timestamp):
    """Query table state at a specific timestamp."""
    
    delta_path = f"{DELTA_CONFIG['base_path']}/{table_name}"
    
    df = spark.read \
        .format("delta") \
        .option("timestampAsOf", timestamp) \
        .load(delta_path)
    
    return df

# Example usage
# historical_df = query_table_at_timestamp("orders", "2024-01-15 10:00:00")
# display(historical_df.limit(10))

# COMMAND ----------

# MAGIC %md
# MAGIC ## Summary
# MAGIC 
# MAGIC This notebook demonstrates a production-ready CDC pipeline that:
# MAGIC 
# MAGIC 1. **Handles 500+ tables** efficiently with intelligent batching and prioritization
# MAGIC 2. **Uses Spark 4's enhanced DataSource API** for optimal streaming performance
# MAGIC 3. **Implements real-time mode** with sub-second latency when needed
# MAGIC 4. **Provides exactly-once semantics** through checkpointing
# MAGIC 5. **Processes data every 15 minutes** with configurable intervals
# MAGIC 6. **Includes comprehensive monitoring** and error handling
# MAGIC 7. **Optimizes Delta tables** automatically for query performance
# MAGIC 8. **Scales horizontally** with parallel processing
# MAGIC 
# MAGIC The solution is designed to handle enterprise-scale CDC workloads with minimal latency and maximum reliability.